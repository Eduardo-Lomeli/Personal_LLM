{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿CUDA disponible?: True\n",
      "Versión de Torch: 2.6.0+cu124\n",
      "Dispositivo actual: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"¿CUDA disponible?:\", torch.cuda.is_available())\n",
    "print(\"Versión de Torch:\", torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Dispositivo actual:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import BasicTokenizer\n",
    "\n",
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.load(model_file=\"../output/tokenizer/my_tokenizer.model\")\n",
    "\n",
    "\n",
    "def get_vocab_size(tokenizer: BasicTokenizer) -> int:\n",
    "    vocab = tokenizer.vocab\n",
    "    special_tokens = tokenizer.special_tokens\n",
    "\n",
    "    return len(vocab) + len(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25ce17c10f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(3647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.92641 M parameters\n"
     ]
    }
   ],
   "source": [
    "from transformer.model import GPTLanguageModel\n",
    "\n",
    "block_size = 512\n",
    "n_embd = 512\n",
    "n_head = 32\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "batch_size = 16\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    dropout=dropout,\n",
    "    device=device\n",
    ").to(device)\n",
    "#model = torch.compile(model)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "919061"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../output/combined_text.txt\", \"r\") as f:\n",
    "    text_sequence = f.read()\n",
    "\n",
    "encoded_text_sequence = tokenizer.encode(text_sequence)\n",
    "len(encoded_text_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split it into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encoded_text_sequence, dtype=torch.long)\n",
    "split_index = int(0.9*len(data))\n",
    "train_data = data[:split_index]\n",
    "val_data = data[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def get_batch(split: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    index = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in index])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in index])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 512]), torch.Size([16, 512]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_batch('train')\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "eval_iters = 200\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss() -> Dict:\n",
    "    output = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            _, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        output[split] = losses.mean()\n",
    "    model.train()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model: GPTLanguageModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    file_path: str = \"checkpoint.pth\"\n",
    ") -> None:\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 7.0319, val loss 7.0368\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m iteration % eval_interval == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m iteration == max_iters - \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m         losses = \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     14\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mval loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m         )\n\u001b[32m     18\u001b[39m         train_losses.append(losses[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eduar\\miniconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mestimate_loss\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[32m     13\u001b[39m     x, y = get_batch(split)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     _, loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     losses[k] = loss.item()\n\u001b[32m     16\u001b[39m output[split] = losses.mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eduar\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eduar\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Mi unidad\\6to Semestre Ceti\\Inteligencia artificial\\LLM\\Personal_LLM\\Personal_LLM\\notebooks\\..\\transformer\\model.py:136\u001b[39m, in \u001b[36mGPTLanguageModel.forward\u001b[39m\u001b[34m(self, input_tokens, targets)\u001b[39m\n\u001b[32m    133\u001b[39m positional_embedding = \u001b[38;5;28mself\u001b[39m.position_embedding_table(\n\u001b[32m    134\u001b[39m     torch.arange(T, device=\u001b[38;5;28mself\u001b[39m.device))\n\u001b[32m    135\u001b[39m x = token_embedding + positional_embedding\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m x = \u001b[38;5;28mself\u001b[39m.final_layer_norm(x)\n\u001b[32m    138\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.final_linear_layer(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eduar\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eduar\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eduar\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eduar\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eduar\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Mi unidad\\6to Semestre Ceti\\Inteligencia artificial\\LLM\\Personal_LLM\\Personal_LLM\\notebooks\\..\\transformer\\model.py:87\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.feed_forward(\u001b[38;5;28mself\u001b[39m.layer_norm_2(x))\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eduar\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eduar\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Mi unidad\\6to Semestre Ceti\\Inteligencia artificial\\LLM\\Personal_LLM\\Personal_LLM\\notebooks\\..\\transformer\\model.py:46\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     out = torch.cat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.heads], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     47\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.projection(out))\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eduar\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eduar\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\Mi unidad\\6to Semestre Ceti\\Inteligencia artificial\\LLM\\Personal_LLM\\Personal_LLM\\notebooks\\..\\transformer\\model.py:23\u001b[39m, in \u001b[36mHead.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     21\u001b[39m _, T, _ = x.shape\n\u001b[32m     22\u001b[39m k = \u001b[38;5;28mself\u001b[39m.key(x)   \u001b[38;5;66;03m# (B,T,hs)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m q = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B,T,hs)\u001b[39;00m\n\u001b[32m     24\u001b[39m weights = q @ k.transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m) * k.shape[-\u001b[32m1\u001b[39m]**-\u001b[32m0.5\u001b[39m\n\u001b[32m     25\u001b[39m weights = weights.masked_fill(\u001b[38;5;28mself\u001b[39m.tril[:T, :T] == \u001b[32m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-inf\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eduar\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eduar\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eduar\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "eval_interval = 10\n",
    "learning_rate = 3e-4\n",
    "save_interval = 100\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    if iteration % eval_interval == 0 or iteration == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(\n",
    "            f\"step {iteration}: \"\n",
    "            f\"train loss {losses['train']:.4f}, \"\n",
    "            f\"val loss {losses['val']:.4f}\"\n",
    "        )\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iteration % save_interval == 0:\n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            epoch=iteration,\n",
    "            loss=loss.item(),\n",
    "            file_path=f\"../output/pre_training/run_2/checkpoint_{iteration}.pth\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Evaluation Step\")\n",
    "plt.ylim(0)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola feaes Est/06/25, 2chid] Ceci Lomeli: �4:50] Eduardo Lomeli: n a as c] Pauline: Sa de 0] Eduardo Lomeli: 1:5a i�avi2:1ítr/05/24, 10:3Me er �estal dJaj2:0 en2:5o\n",
      "[2est\n",
      "[05derLa /21, 2e  tr/09 man:3�\n"
     ]
    }
   ],
   "source": [
    "input_tokens = tokenizer.encode(\"Hola fea\")\n",
    "input_tokens = torch.tensor(\n",
    "    input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_tokens=input_tokens, max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.decode(output[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vincent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
