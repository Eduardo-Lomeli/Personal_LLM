{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downaload a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, you need to login to be able to download a dataset. Run this cell, if that is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Huggingface, click on the copy button to get the name of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset_from_hugging_face](../images/get_dataset_from_hugging_face.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And provide that name to `load_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7aa5ef923049f3bc300b41e9187ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eduar\\miniconda3\\envs\\vincent\\lib\\site-packages\\huggingface_hub\\file_download.py:121: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Eduar\\.cache\\huggingface\\hub\\datasets--opus_books. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd596922f49545e38d0a552961ad439f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-es/train-00000-of-00001.parquet:   0%|          | 0.00/16.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b294a521e344265bc4ff92c2de7cd3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/93470 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"opus_books\", \"en-es\", split=\"train\")\n",
    "\n",
    "book_lines = []\n",
    "limit_lines = 50000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset_from_hugging_face](../images/get_dataset_from_hugging_face.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the sequence of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the splits into a single text sequence. In [AtlaSet](https://huggingface.co/datasets/atlasia/Atlaset), there were only two splits (train and test), but if a validation set is included, add an extra loop to append its text to the data list as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tqdm(rows):\n\u001b[0;32m      7\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(row)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "data = []\n",
    "rows = dataset[\"train\"][\"text\"]\n",
    "for row in tqdm(rows):\n",
    "    data.append(row)\n",
    "\n",
    "rows = dataset[\"test\"][\"text\"]\n",
    "for row in tqdm(rows):\n",
    "    data.append(row)\n",
    "\n",
    "print(len(\" \".join(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally store the sequence of text on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/AtlaSetCombined.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando 93470 líneas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93470/93470 [00:03<00:00, 27859.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total caracteres: 11703621\n",
      " Archivo guardado en: ../data/OPUSCombined.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "# 1. Detectar si 'dataset' es un diccionario (DatasetDict) o ya es el split (Dataset)\n",
    "# Si cargaste con split=\"train\", usa 'dataset' directamente.\n",
    "# Si cargaste sin split, usa 'dataset[\"train\"]'.\n",
    "if hasattr(dataset, \"keys\") and \"train\" in dataset.keys():\n",
    "    iterable_data = dataset[\"train\"]\n",
    "else:\n",
    "    iterable_data = dataset\n",
    "\n",
    "print(f\"Procesando {len(iterable_data)} líneas...\")\n",
    "\n",
    "# 2. Iterar y extraer el español\n",
    "for row in tqdm(iterable_data):\n",
    "    # En opus_books, el texto está dentro de un diccionario 'translation'\n",
    "    spanish_text = row['translation']['es']\n",
    "    data.append(spanish_text)\n",
    "\n",
    "# Nota: opus_books NO suele tener split \"test\", así que omitimos ese bloque\n",
    "# para evitar errores de KeyError.\n",
    "\n",
    "print(f\"Total caracteres: {len(' '.join(data))}\")\n",
    "\n",
    "# 3. Guardar\n",
    "output_path = \"../data/OPUSCombined.txt\" # Asegúrate que la carpeta 'data' exista\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\" \".join(data))\n",
    "\n",
    "print(f\" Archivo guardado en: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vincent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
